# Моделирование и исследование случайных процессов на примере временных рядов
## Цель проекта

Цель данного проекта – сравнить различные подходы моделирования временных рядов для прогнозирования стохастических процессов. Основное внимание уделяется эффективности моделей с точки зрения:
* трейдинговой стратегии (суммарный профит),
* классических метрик ошибок (MAE, MSE, ME).

### Используемые модели

1. Простые математические модели: Naive, Holt
2. Статистические модели: ARIMA
3. Модели классического машинного обучения: Linear Regression, XGBoost
4. Модели с feature-engineering: LR_Features, XGB_Features (скользящие окна, EMA, lag-значения)

* Также были проведены эксперименты с рекуррентной нейросетью GRU, и стекингом над градиентным бустингом, обе модели потребовали существенного увелечения датасета  (чтобы добиться сопостовимого качества с другими моделями пришлось увеличить датасет в ~30 раз) и непропорционально высокий (относительно других моделей) объем     вычислений. Так как вышеуказанные не позволили добиться существенного улучшения метрик, был сделан вывод о нецелесообразности их использования.

## Стохастические процессы

Используются три стохастических процесса:
1. Броуновское движение (BM) – классическая модель случайного блуждания.
2. Геометрическое Броуновское движение (GBM) – моделирует случайные движения с экспоненциальным ростом и волатильностью.
3. Орнштейн-Уленбек (OU) – процесс с возвратом к среднему, моделирующий ценовые ряды с коррекцией.

### Визуализация:
1. Полный ряд данных
2. Разделение на train/test (красная линия)
3. Наложение сглаженного ряда для наглядности
###
![временной ряд, основанный на броуновском движении](/pics/picture1_1.png)
###
![временной ряд, основанный на геометрическом броуновском движении](/pics/picture1_2.png)
###
![временной ряд, основанный на процессе Орнштейна-Уленбека](/pics/picture1_3.png)
###
## Ход работы


### Подготовка данных

Для построения моделей были использованы два типа датасетов:
1. Lagged dataset – для каждого момента времени создаются лаги y_i-1, …, y_i-lags.
2. Feature dataset – расширенные признаки, включающие:
* скользящие окна (mean, min, max, var)
* экспоненциальное сглаживание (EMA)
* lag-значения

Принцип: для предсказания y_i используются значения из прошлого (y_{i-1} и другие лаги), чтобы избежать утечки информации.
Train/test разбиение происходит после формирования признаков, чтобы модель обучалась только на исторических данных.

### Использованные Модели

* Naive	— Прогноз следующей цены равен текущей.

* ARIMA	— Авторегрессия + интегрирование + скользящее среднее.

* Linear Regression / XGBoost — Модели на лагированных данных (lag_1, ..., lag_n).

* Holt — Экспоненциальное сглаживание с трендом.

* LR_Features / XGB_Features — Модели с дополнительными фичами (скользящие окна, EMA, лаги).

### Использованные Метрики

1. Финансовые:
* Прибыль / суммарный профит по стратегии на тестовой выборке
2. Классические метрики:
* MAE (Mean Absolute Error)
* MSE (Mean Squared Error)
* ME (Mean Error)

### Эксперименты

Множественные симуляции:
* 500 симуляций для оценки средних значений
* Сводные таблицы: средний профит каждой модели по процессу

Сравнение режимов:

* Raw vs Smoothed (только для обучающих данных):
* Средние метрики (Profit, MAE, MSE) для каждой модели в двух режимах

Пример:

![метрики для линейной регрессии, в сглаженном и сыром режимах](/pics/picture2.png)

## Результирующая таблица результатов

![таблица результатов1](/pics/picture2.png)
![таблица результатов2](/pics/picture4.png)
![таблица результатов3](/pics/picture5.png)

# Выводы

## Броуновское движение (BM)
Все модели показывают около нулевую прибыль, что ожидаемо, так как процесс не имеет тенденций и сезонности и полностью основан на случайном шуме.
Ошибки MAE и MSE остаются небольшими для моделей с feature-engineering, но трейдинговый профит низок у всех подходов.
## Геометрическое Броуновское движение (GBM)
Общий профит низкий для всех моделей.
ARIMA и XGBoost дают отрицательный профит, что кажется неожиданным, учитывая наличие тренда в процессе. Возможные причины:
Для ARIMA могли быть недостаточно длинные лаги; увеличение лагов ограничено вычислительной сложностью.
XGBoost ориентируется на локальные колебания, а не на глобальный тренд, из-за структуры деревьев решающих правил, что ограничивает обобщение на тестовой выборке.
Наиболее стабильные результаты показали Holt и Linear Regression, что соответствует теории: Holt хорошо моделирует тренды, а линейная регрессия захватывает общую зависимость между лагами.
## Орнштейн-Уленбек (OU)
Наибольший профит достигнут у Holt и XGB_Features.
OU-ряд имеет явный возврат к среднему, что облегчает моделям задачу прогнозирования: признаки в XGB остаются в диапазоне, известном модели из обучающей выборки, что повышает способность обобщения на тестовую выборку.

### Влияние feature-engineering
Теоретически, модели с расширенными признаками должны показывать лучшие метрики за счет большей информативной нагрузки.
На практике улучшение метрик не всегда наблюдается, что может быть связано с особенностями стохастических процессов и тем, что некоторые модели (например, XGBoost) фокусируются на локальных паттернах, а не на глобальном поведении ряда.

### Влияние сглаживания 
Сглаживание обучающих данных теоретически должно уменьшать влияние шума, повышать устойчивость моделей и снижать риск переобучения.
На практике результаты смешанные:
Для некоторых моделей (например, Holt, Linear Regression) сглаживание улучшает метрики MAE/MSE.
Для моделей, ориентированных на локальные колебания (например, XGBoost), сглаживание может уменьшать трейдинговый профит, так как часть полезной информации о краткосрочных изменениях теряется.

## Общие выводы
Модели, учитывающие тренды и feature-engineering, лучше справляются с процессами, где существует среднее возвращение (OU) или явный тренд (GBM).
На полностью случайных процессах (BM) ни одна модель не приносит значимого профита, что подтверждает теоретические ожидания.
Сглаживание обучающих данных улучшает робастность моделей к шуму, но не всегда повышает прибыльность трейдинговых стратегий.
Выбор модели должен основываться на характере ряда:
Трендовые и mean-reverting ряды: Holt и XGB_Features.
Высоковолатильные случайные ряды: feature-engineered модели дают более стабильные ошибки, но трейдинговый профит ограничен.
